#Reducing False Positives in Static Bug Detection with LLMs: An Empirical Study in Industry
In this study, we conduct a systematic investigation into the prevalence of false positives generated by static analyzers in enterprise environments and evaluate the effectiveness of LLM–based false positive reduction techniques in industrial practice.

##Dataset Construction

We collect data from the real-world static bug detection pipeline at Tencent, which includes the bug alarms reported by its enterprisecustomized SAT (i.e., BPcheck) on the large-scale software of its Advertising and Marketing Services (AMS) business line (i.e., over hundreds of components). Based on the historical data, we construct a real-world industry false alarm reduction dataset of 433 bug alarms (i.e., 328 false positives and 105 true positives), which covers the three most frequent bug types (i.e., Null Pointer Dereference (NPD), Out-of-Bounds (OOB), and Divide-by-Zero (DBZ) reported by BPcheck in Tencent. Based on our dataset, we empirically study the performance of LLMs in reducing false alarms for industrial software by answering the following research questions.

Our dataset cannot be released due to Tencent confidentiality, all explored methods and detailed results are as follows.

##Research Questions

###RQ1 (Prevalence of false positives in industry)

We first perform interviews with Tencent developers to investigate the application practice of SATs. SATs serve as a critical gated check-in during the continuous integration process at Tencent, and all the reported bug alarms would be manually reviewed by developers, with false alarms proceeding to a second-round manual validation to avoid missing any potential bugs. False alarms are not only prevalent (i.e., over 76%), but also require massive manual inspection effort (i.e., on average 10 - 20 minutes per alarm). 

### RQ2 (Effectiveness of LLMs in reducing false alarms)

We further evaluate a diverse range of different LLM-based false alarm reduction techniques on Tencent datasets, including (1) basic LLMs, (2) different advanced prompting strategies, (3) hybrid techniques of static analysis and LLMs. Moreover, we also compare LLM-based techniques with traditional learning-based techniques on Tecent datasets. To the best ofour knowledge, this is the most comprehensive evaluation on learning-based false alarm reduction ofindustrial software. Our experimental results show the strong potential of LLM-based techniques in reducing false positives for industrial software, e.g., LLMPFA, the hybrid techniques of static analysis and LLMs, eliminates 94%–98% of false positives across different backbone LLMs while maintaining high recall.

### RQ3 (Cost analysis)

We measure the time and money costs of LLM-based false alarm reduction techniques when applied on Tencent datasets. In particular, the average time cost per alarm ranges from 2.1 to 109.5 seconds, and the average expense per alarm is 0.0011$–0.12$. Such acceptable time and money costs make LLMbased techniques especially valuable for supporting static false positive reduction in industrial practice, substantially reducing manual inspection effort (i.e., 10 - 20 minutes). 

### RQ4 (Breakdown and case analysis)

We further perform breakdown analysis and find the effectiveness variety ofLLM-based false alarm reduction techniques across different bug categories. In particular, we find that most LLM-based techniques perform best on Divide-by-Zero (DBZ) bugs, while their performance is lowest on Null Pointer Dereference (NPD) bugs. Moreover, we perform bad case analysis to identify the limitations of LLMs when applied to enterprise-scale bug detection, such as handling bugs of long code contexts, reasoning complex cascaded constraints, and insufficient semantic understanding. These challenges remain future directions for advancing the effectiveness ofLLMs-based false alarm reduction in industry.

##Codes & Results

[result](https://github.com/LLM4SFPR/static-bug-false-positives-reduction-in-practice/tree/main/result) contains test results of LLM4PFA method, all in JSON format. We used 4 LLMs (GPT, Deepseek, claude and qwen) and tested on 3 vulnerability types (DivideByZero, Nullpointer and OutOfBound). We removed fields containing code information, leaving only labels and test results.

[baseline/DL-based](https://github.com/LLM4SFPR/static-bug-false-positives-reduction-in-practice/tree/main/baseline/DL-based) contains codes of 4 deep learning-based baseline methods we used (sensitive information removed).

[baseline/LLM-based](https://github.com/LLM4SFPR/static-bug-false-positives-reduction-in-practice/tree/main/baseline/LLM-based) contains codes of 5 LLM-based baseline methods we used (sensitive information removed), and JSON files containing test results (only labels and test results are left).